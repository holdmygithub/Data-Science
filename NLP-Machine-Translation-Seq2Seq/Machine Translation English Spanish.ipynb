{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "variable-piece",
   "metadata": {},
   "source": [
    "# Machine Translation -  English to Spanish - Seq2Seq\n",
    "\n",
    "\n",
    "#### Here we aim to create a english to spanish translation machine using encoder decoder models.\n",
    "\n",
    "#### Data credits: http://www.manythings.org/anki/spa-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-bargain",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 10000 #Regulating the training set for faster training\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "MAX_SEQ_LEN = 20\n",
    "EMBEDDING_DIM = 300\n",
    "LSTM_UNITS = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-priority",
   "metadata": {},
   "source": [
    "### Importing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_spa = \"spa.txt\"\n",
    "\n",
    "english_text = [] #used in encoder\n",
    "spanish_text_in = [] #used in decoder\n",
    "spanish_text_out = [] #used in decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path_spa)\n",
    "i = 0\n",
    "for line in f:\n",
    "    i+=1\n",
    "    if(i>NUM_SAMPLES):\n",
    "        break\n",
    "    if '\\t' in line:\n",
    "        eng,spa,_ = line.split('\\t')\n",
    "        english_text.append(eng)\n",
    "        spanish_text_in.append(\"<sos> \"+spa)\n",
    "        spanish_text_out.append(spa+\" <eos>\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-navigator",
   "metadata": {},
   "source": [
    "**Printing some of the the appended texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_text_in[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_text_out[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-theology",
   "metadata": {},
   "source": [
    "### Tokenizing the texts and Padding them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-aggregate",
   "metadata": {},
   "source": [
    "**Tokenizer for encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_encoder = Tokenizer(num_words = MAX_VOCAB_SIZE)\n",
    "tokenizer_encoder.fit_on_texts(english_text)\n",
    "english_text = tokenizer_encoder.texts_to_sequences(english_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-hamburg",
   "metadata": {},
   "source": [
    "**Finding the maximum sequence length in english_text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len_text = max(len(s) for s in english_text)\n",
    "max_seq_len_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len_eng = min(max_seq_len_text,MAX_SEQ_LEN)\n",
    "max_seq_len_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-vancouver",
   "metadata": {},
   "source": [
    "**Padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text = pad_sequences(english_text,maxlen=max_seq_len_eng,padding='pre')\n",
    "english_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_vocab_size = min(MAX_VOCAB_SIZE,len(tokenizer_encoder.word_index)+1)\n",
    "encoder_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-spiritual",
   "metadata": {},
   "source": [
    "**Tokenizer for decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_decoder = Tokenizer(num_words=MAX_VOCAB_SIZE,filters='')\n",
    "tokenizer_decoder.fit_on_texts([\"<eos>\"] + spanish_text_in)\n",
    "spanish_text_in = tokenizer_decoder.texts_to_sequences(spanish_text_in)\n",
    "spanish_text_out = tokenizer_decoder.texts_to_sequences(spanish_text_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-device",
   "metadata": {},
   "source": [
    "**Finding the maximum sequence length in spanish_text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len_text = max(len(s) for s in spanish_text_in)\n",
    "max_seq_len_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len_spa = min(max_seq_len_text,MAX_SEQ_LEN)\n",
    "max_seq_len_spa "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-accessory",
   "metadata": {},
   "source": [
    "**Padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_text_in = pad_sequences(spanish_text_in,maxlen=max_seq_len_spa,padding=\"post\")\n",
    "spanish_text_out = pad_sequences(spanish_text_out,maxlen=max_seq_len_spa,padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_vocab_size = min(MAX_VOCAB_SIZE,len(tokenizer_decoder.word_index)+1)\n",
    "decoder_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-dragon",
   "metadata": {},
   "source": [
    "### Creating Embedding matrix for Encoder (English Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_word_index = tokenizer_encoder.word_index\n",
    "eng_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-automation",
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_word_index = tokenizer_decoder.word_index\n",
    "spa_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((encoder_vocab_size,EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading spacy model for english language\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating embedding matrix \n",
    "for word,index in eng_word_index.items():\n",
    "    if(index < encoder_vocab_size):\n",
    "        vector = nlp(word).vector\n",
    "        embedding_matrix[index] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-redhead",
   "metadata": {},
   "source": [
    "### One hot encoding the target sequence - spanish_text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_spanish_text_out = np.zeros((len(spanish_text_out),max_seq_len_spa,decoder_vocab_size))\n",
    "\n",
    "for seq_i, seq in enumerate(spanish_text_out):\n",
    "    for word_i,word in enumerate(seq):\n",
    "        if(word>0):\n",
    "            ohe_spanish_text_out[seq_i,word_i,word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-project",
   "metadata": {},
   "source": [
    "### Creating the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM,Dense,Input,Embedding\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-pizza",
   "metadata": {},
   "source": [
    "**Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(max_seq_len_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-wichita",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Embedding(encoder_vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],input_length = max_seq_len_eng)(encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_enc = LSTM(LSTM_UNITS,activation='relu',return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_out,h,c = lstm_enc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_states = [h,c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-search",
   "metadata": {},
   "source": [
    "**Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(max_seq_len_spa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_embedding = Embedding(decoder_vocab_size,EMBEDDING_DIM,input_length=max_seq_len_spa)\n",
    "x = dec_embedding(decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_dec = LSTM(LSTM_UNITS,activation='relu',return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,_,_ = lstm_dec(x,initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_dec = Dense(decoder_vocab_size,activation=\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_out = dense_dec(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_input,decoder_input],dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit([english_text,spanish_text_in],ohe_spanish_text_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-pattern",
   "metadata": {},
   "source": [
    "### Creating the sampling model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-lodging",
   "metadata": {},
   "source": [
    "**Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_enc_model = Model(encoder_input,encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(s_enc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-sixth",
   "metadata": {},
   "source": [
    "**Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_decoder_input = Input(shape=(1))\n",
    "s_decoder_input_h = Input(shape=(LSTM_UNITS))\n",
    "s_decoder_input_c = Input(shape=(LSTM_UNITS))\n",
    "\n",
    "s_decoder_states = [s_decoder_input_h,s_decoder_input_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dec_embedding(s_decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,h,c = lstm_dec(x,initial_state=s_decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_decoder_out = dense_dec(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dec_model = Model(inputs=[s_decoder_input]+s_decoder_states,outputs=[s_decoder_out] + [h,c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-blair",
   "metadata": {},
   "source": [
    "#### Defining the translate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverse indexing required to convert token indices to words\n",
    "spa_index_word = { s:v for v,s in spa_word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(english_sentence):\n",
    "    output_spa = []\n",
    "    \n",
    "    x = tokenizer_encoder.texts_to_sequences([english_sentence])\n",
    "    x = pad_sequences(x,maxlen=max_seq_len_eng)\n",
    "    \n",
    "    h,c = s_enc_model.predict(x)\n",
    "    next_states = [h,c]\n",
    "    \n",
    "    next_word = np.array([spa_word_index['<sos>']])\n",
    "    \n",
    "    for i in range(max_seq_len_spa):\n",
    "        next_word, h,c = s_dec_model.predict([next_word] + next_states)\n",
    "        \n",
    "        next_word = np.array([np.argmax(next_word)])\n",
    "        next_states = [h,c]\n",
    "        \n",
    "        append_word = spa_index_word[next_word[0]]\n",
    "        if(append_word == \"<eos>\"):\n",
    "            break\n",
    "            \n",
    "        output_spa.append(append_word)\n",
    "        \n",
    "    return \" \".join(output_spa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"hi good morning\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
